# ğŸ§  MediBot â€“ AI-Powered Medical Chatbot with RAG using LangChain and Hugging Face

MediBot is an AI-powered chatbot built using **LangChain**, **Hugging Face**, **FAISS**, and **Streamlit**. It leverages **Retrieval-Augmented Generation (RAG)** to answer medical queries based **only on uploaded medical PDFs**, avoiding hallucinations and ensuring responses are grounded in real, verified content.

---

## ğŸ“Œ Features

- ğŸ’¬ Ask medical questions in natural language
- ğŸ“š Answers are retrieved from your uploaded PDF documents
- ğŸ” Uses **FAISS** for fast vector-based semantic search
- ğŸ¤– Uses **Hugging Face Transformers** for LLM responses
- ğŸ“„ Preprocesses large PDFs into chunks for accurate retrieval
- ğŸ§  Embeds chunks using Sentence Transformers

---

## ğŸ› ï¸ Tech Stack

| Tool/Library      | Purpose                             |
|-------------------|-------------------------------------|
| `LangChain`       | Building RAG pipelines              |
| `FAISS`           | Storing and querying vector DB      |
| `HuggingFace`     | Hosting LLMs and embedding models   |
| `Streamlit`       | Web UI for interaction              |
| `Python` + `venv` | Core development & environment mgmt |

---

## ğŸ“‚ Project Structure

MediBot/
â”œâ”€â”€ data/                                        # Folder containing input PDFs
â”œâ”€â”€ vectorstore/                                 # FAISS vector database
â”‚ â””â”€â”€ db_faiss/                                  # Stored embeddings
â”œâ”€â”€ create_memory_for_llm.py                     # Script to load PDFs & store vector DB
â”œâ”€â”€ connect_memory_with_llm.py                   # Query LLM with vector DB (RAG)
â”œâ”€â”€ .env                                         # Hugging Face API key
â””â”€â”€ README.md                                     # This file







## âš™ï¸ Setup Instructions

1. Create and Activate a Virtual Environment

python -m venv venv
venv\Scripts\activate      # Windows


2. Install Dependencies
 
# pip install -r requirements.txt

3. Add Your Hugging Face Token
Create a .env file in the root directory and add:

# HF_TOKEN=your_huggingface_api_token_here

You can get a free token at: https://huggingface.co/settings/tokens

## ğŸ§  Step 1: Create the Vector Memory (FAISS Index)

Run this script to load your PDFs, chunk text, generate embeddings, and save to FAISS:

python create_memory_for_llm.py

it will-
âœ… Loads PDFs from data/
âœ… Splits text into chunks
âœ… Embeds each chunk
âœ… Saves embeddings into FAISS (vectorstore/db_faiss)

## ğŸ” Step 2: Interact with the Chatbot (RAG + LLM)
Once the FAISS vector store is ready, run:

python connect_memory_with_llm.py

Ask medical questions and get LLM responses grounded in your PDFs.

ğŸŒ Optional: Run the Web App
Launch the chatbot interface via Streamlit:

streamlit run app.py

Make sure app.py is configured to load the correct FAISS DB and embedding model.


ğŸ§ª Sample Query
Input:
# What are the symptoms of malaria?

Response:
# Malaria symptoms typically include high fever, chills, sweating, headache, nausea, and muscle pain. These symptoms often appear 10â€“15 days after the bite of an infected mosquito.

## ğŸ“¦ requirements.txt
langchain>=0.2.0
langchain-community>=0.2.0
langchain-huggingface>=0.0.1
huggingface_hub>=0.21.4
faiss-cpu>=1.7.4
sentence-transformers>=2.6.1
python-dotenv>=1.0.1
streamlit>=1.35.0

# Install using:

pip install -r requirements.txt

## ğŸ“„ Dataset Notes
Source: The Gale Encyclopedia of Medicine

Place your PDF files in the data/ folder.

Ensure PDFs are well-formatted (not scanned images) for accurate text extraction and chunking.

## âš ï¸ Important Notes

-FAISS DB is local and persistent
-Responses are limited to your PDF data â€” no external knowledge used
-Works offline after embedding stage (Hugging Face API still required during query)
-Use from a trusted environment â€” deserialization involves pickle




















































